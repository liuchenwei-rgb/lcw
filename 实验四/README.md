# 实验四 ：PyTorch环境下基于MNIST数据集的手写数字识别实验报告

## 一、 实验目的

学习在Python环境中配置PyTorch深度学习框架及相关依赖库。
学习使用PyTorch的数据加载工具（torchvision.datasets和 DataLoader）准备和预处理MNIST数据集。
理解并实践构建一个简单的多层感知机（MLP）神经网络模型。
掌握模型训练、验证的基本流程，包括前向传播、损失计算、反向传播和参数优化。
通过观察训练过程中的损失（loss）和准确率（acc）变化，分析模型的学习效果。

## 二、 实验环境

操作系统：Windows
编程语言：Python
深度学习框架：PyTorch 2.9.1
核心依赖库：
torch2.9.1
torchvision（用于加载MNIST数据集）
numpy2.2.6
matplotlib3.10.7
实验数据：MNIST手写数字数据集（训练集60,000张，测试集10,000张，图像尺寸28x28）。

## 三、 实验原理与方法

模型设计：本实验采用一个简单的两层全连接神经网络（MLP）。输入层将28x28的图像展平为784维向量，经过一个具有128个神经元的隐藏层（使用ReLU激活函数），最后输出到10个神经元的输出层（对应0-9十个数字类别）。输出使用交叉熵损失函数（F.cross_entropy）进行计算，其内部已集成Softmax操作。
优化方法：使用随机梯度下降（SGD）作为优化器，学习率设置为0.1。
训练流程：每次迭代（epoch）遍历整个训练集。每个批次（batch_size=2048）数据经过前向传播得到预测值，计算损失后执行反向传播更新模型参数。每个epoch结束后，在独立的测试集上评估模型性能，计算平均损失和分类准确率。
## 四、 实验内容与步骤

环境搭建与数据准备：
在Python虚拟环境中，通过pip成功安装PyTorch、NumPy、Matplotlib等必要库。
通过torchvision.datasets.MNIST接口自动下载MNIST数据集，并保存在./data目录下。使用DataLoader进行批量加载和数据打乱。
模型构建：
使用nn.Module基类定义MLP模型，在__init__中初始化两个线性层（nn.Linear）。
在forward方法中定义数据流向：输入 -> 线性层1 -> ReLU激活 -> 线性层2 -> 输出。
模型训练与测试：
初始化模型、优化器（SGD）和损失函数。
进行10个epoch的训练循环。每个epoch内：
将模型设置为训练模式（model.train()），遍历训练集，计算损失并更新权重。
将模型设置为评估模式（model.eval()），在测试集上计算损失和准确率，不更新权重。

实验核心代码
模型定义与训练代码实现
本实验的核心代码分为三个主要部分：数据加载、模型定义、以及训练测试循环。具体实现如下。
1. 数据加载与环境配置
此部分代码负责导入必要的库、设置超参数，并加载MNIST数据集。

<img width="597" height="542" alt="1a691030b487d222013e4b1be41f5ed9" src="https://github.com/user-attachments/assets/da5ae6d1-e07a-473c-a143-1de0c19638a8" />


2. MLP模型定义
此部分代码定义了一个简单的两层全连接神经网络（MLP）。

<img width="758" height="578" alt="9b15b7ea0ad8817b1104e16708ddc1a8" src="https://github.com/user-attachments/assets/9a948aeb-9a92-42ae-beb5-054188a499f0" />


3. 模型训练与评估循环

此部分代码实现了完整的训练与测试流程，包括每个epoch的前向传播、损失计算、反向传播、参数更新以及在测试集上的性能评估。

<img width="629" height="326" alt="6febba902820ed1613d2a18234b35dce" src="https://github.com/user-attachments/assets/fa898032-9615-4a02-afe7-fadce4cd533c" />

## 五、 实验结果与分析
训练过程记录：
模型训练了10个epoch，每个epoch在测试集上的损失和准确率记录如下：

<img width="236" height="127" alt="403c55cdfdcf8c8786ff7c820e2ae917" src="https://github.com/user-attachments/assets/b7df5c0a-58dd-4c48-b2ad-c213c1ee5e57" />

## 六、 实验总结
本次实验的核心任务是使用PyTorch框架，构建并训练一个多层感知机（MLP）模型，以完成对MNIST手写数字图像的分类

我遇到了几个典型且值得记录的问题。

第一个困难出现在模型定义阶段。​ 在编写MLP类的初始化方法__init__时，我下意识地按照字典的语法，在nn.Linear的参数里使用了冒号（in_features: 784）。这直接导致程序报出“SyntaxError”（语法错误），整个模型都无法创建。我一开始有些困惑，因为代码逻辑看起来是对的。经过仔细检查并与官方文档对比后，才发现PyTorch中定义层参数使用的是等号（=），而不是冒号。

第二个困难更具隐蔽性，发生在数据加载部分。​ 代码在运行时没有报错，但训练一开始，损失就居高不下，准确率远低于预期。我反复检查了模型结构、损失函数和优化器，都没有发现问题。最后，我怀疑问题出在数据本身。通过打印训练集的形状train_loader.dataset.data.shape，我确认数据是正确加载的。直到我逐行检查DataLoader的定义时，才发现了一个拼写错误：在transforms.Compose前面，我写的参数名是transforms=（复数形式）。而PyTorch datasets.MNIST的正确参数名是transform=（单数形式）。这意味着我自定义的ToTensor转换根本就没有被应用，模型接收到的可能是原始像素值范围（0-255）的图像，而不是归一化后的张量。将这个拼写错误改正后，模型立刻开始了正常的学习，

第三个挑战是对训练流程的理解。​ 在写训练循环时，我一开始忘记了在每次参数更新前调用optimizer.zero_grad()来清零梯度。这导致梯度在不同批次间不断累积，模型参数更新混乱，完全无法收敛。在查阅教程后，我才明白PyTorch的梯度是累加的，必须在每次反向传播前手动清零，这是与一些其他框架不同的设计

总的来说，这次实验让我真正动手走通了一个深度学习项目的全流程：数据加载、模型搭建，到训练循环、性能评估。
